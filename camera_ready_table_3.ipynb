{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import SR1, BFGS\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import confound_mdp\n",
    "import confound_ope\n",
    "import confound_env\n",
    "\n",
    "from core.sepsisSimDiabetes.State import State\n",
    "from core.sepsisSimDiabetes.Action import Action\n",
    "from core import generator_confounded_mdp as DGEN\n",
    "from core import conf_wis as CWIS\n",
    "from core import loss_minimization as LB\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = []\n",
    "# # each row:\n",
    "# #   [mdp , pi_b, pi_e, horizon, gamma, nStates, nActions, term]\n",
    "\n",
    "pi_b, P, R, x_dist, u_dist, gamma = confound_env.toy227(0.25, 0.35)\n",
    "toy = confound_mdp.ConfoundMDP(P, R, x_dist, u_dist, gamma)\n",
    "horizon = 5\n",
    "nStates = P.shape[2]\n",
    "nActions = P.shape[1]\n",
    "pi_e = np.zeros((nStates, nActions))\n",
    "for i in range(nStates):\n",
    "    pi_e[i] = [0.3, 0.7]\n",
    "    \n",
    "envs.append([toy, pi_b, pi_e, horizon, gamma, nStates, nActions, -1])\n",
    "\n",
    "horizon = 4\n",
    "graph_len = 4\n",
    "pi_b, P, R, x_dist, u_dist, gamma = confound_env.graph_opetools(horizon=graph_len, slip=0.25, confound_weight=0.23)\n",
    "R = -1*R\n",
    "graph = confound_mdp.ConfoundMDP(P, R, x_dist, u_dist, gamma)\n",
    "nStates = P.shape[2]\n",
    "nActions = P.shape[1]\n",
    "\n",
    "pi_e = np.zeros((nStates, nActions))\n",
    "for i in range(nStates):\n",
    "    pi_e[i] = [0.3, 0.7]\n",
    "    \n",
    "envs.append([graph, pi_b, pi_e, horizon, gamma, nStates, nActions, -1])\n",
    "\n",
    "horizon = 20\n",
    "pi_b, P, R, x_dist, u_dist, gamma = confound_env.toymc_opetools(n_left=8, n_right=8, horizon=20, slip=0.15, confound_weight=0.6)\n",
    "#R = -1*R\n",
    "toymc = confound_mdp.ConfoundMDP(P, R, x_dist, u_dist, gamma)\n",
    "\n",
    "nStates = P.shape[2]\n",
    "nActions = P.shape[1]\n",
    "\n",
    "pi_e = np.zeros((nStates, nActions))\n",
    "for i in range(nStates):\n",
    "    pi_e[i] = [0.15, 0.85]\n",
    "    \n",
    "envs.append([toymc, pi_b, pi_e, horizon, gamma, nStates, nActions, -1])\n",
    "\n",
    "horizon = 8\n",
    "pi_b, P, R, x_dist, u_dist, gamma = confound_env.gridworld_opetools(horizon = horizon, slip = 0.04, confound_weight=0.6)\n",
    "#R = -1*R\n",
    "gridworld = confound_mdp.ConfoundMDP(P, R, x_dist, u_dist, gamma)\n",
    "\n",
    "nStates = P.shape[2]\n",
    "nActions = P.shape[1]\n",
    "\n",
    "pi_e = np.zeros((nStates, nActions))\n",
    "for i in range(nStates):\n",
    "    pi_e[i] = [0.4, 0.1, 0.4, 0.1]\n",
    "    \n",
    "envs.append([gridworld, pi_b, pi_e, horizon, gamma, nStates, nActions, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in order to work with namkoong et al code need to add t and remove u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(dataset):\n",
    "    n, horizon, _ = dataset.shape\n",
    "    \n",
    "    # dataset: x,a,u,x',r\n",
    "    # keramati data: t,a,x,x',r\n",
    "    new_data = np.zeros((n, horizon, 5))\n",
    "    new_data[:,:,0] = np.arange(horizon)\n",
    "    new_data[:,:,1] = dataset[:,:,1]\n",
    "    new_data[:,:,2] = dataset[:,:,0]\n",
    "    new_data[:,:,3] = dataset[:,:,3]\n",
    "    new_data[:,:,4] = dataset[:,:,4]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare sensitivity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_gamma_mb_envs = []\n",
    "true_gamma_nky_envs = []\n",
    "\n",
    "Pbs = [10, 100000]\n",
    "nPbs = len(Pbs)\n",
    "\n",
    "hadds = [0]\n",
    "\n",
    "for mdp , pi_b, pi_e, base_horizon, gamma, nStates, nActions, term in envs:\n",
    "    \n",
    "    for h in hadds:\n",
    "        horizon = base_horizon + h\n",
    "\n",
    "        print(\"running env...\")\n",
    "        dataset = confound_mdp.collect_sample(200000, mdp, pi_b, horizon)\n",
    "        data = dataset.reshape((dataset.shape[0]*dataset.shape[1],5))\n",
    "        Phat = confound_ope.estimate_P(dataset, mdp)\n",
    "        pihat = confound_ope.estimate_pi(dataset, mdp)\n",
    "        for a in range(nActions):\n",
    "            for s in range(nStates):\n",
    "                if Phat[a,s].sum() == 0:\n",
    "                    Phat[a,s,term] = 1\n",
    "                if pihat[s].sum() == 0:\n",
    "                    pihat[s,:] = 1/nActions\n",
    "        pi_avg = pi_b[0] * u_dist[0] + pi_b[1] * u_dist[1]\n",
    "\n",
    "        print(\"value of pi_e with no confounding\")\n",
    "        Q0 = np.zeros((nStates, nActions))\n",
    "        nom_q = Q0.copy()\n",
    "        for t in range(horizon):\n",
    "            nom_q = confound_ope.fitted_q_update(nom_q, pi_e, dataset, mdp)\n",
    "        print(mdp.get_value(nom_q,pi_e)[1])\n",
    "\n",
    "        nky_data = transform_dataset(dataset)\n",
    "        returns = confound_mdp.calc_returns(dataset, gamma, horizon)\n",
    "       \n",
    "        nky_results = np.zeros((1))            \n",
    "        mb_results = np.zeros((1, nPbs))\n",
    "        \n",
    "        Q0 = np.zeros((nStates, nActions))\n",
    "        q_reparam_samp = Q0.copy()\n",
    "        for t in range(horizon-1):\n",
    "            q_reparam_samp = confound_ope.fitted_q_update(q_reparam_samp, pi_e, dataset, mdp)\n",
    "        V0 = mdp.get_value(q_reparam_samp,pi_e)[0]\n",
    "        \n",
    "        worst_nky = 0\n",
    "        for x in range(nStates):\n",
    "            for a in range(nActions):\n",
    "                for u in range(2):\n",
    "                    ratio1 = pi_b[u, x, a] / (1-pi_b[u, x, a])\n",
    "                    ratio2 = pi_b[1-u, x, a] / (1-pi_b[1-u, x, a])\n",
    "                    ratio_ratio = ratio1/ratio2\n",
    "                    if ratio_ratio > worst_nky:\n",
    "                        worst_nky = ratio_ratio\n",
    "        print(worst_nky)\n",
    "\n",
    "        worst_msm = 0\n",
    "        for x in range(nStates):\n",
    "            for a in range(nActions):\n",
    "                for u in range(2):\n",
    "                    ratio1 = pi_b[u, x, a] / (1-pi_b[u, x, a])\n",
    "                    ratio2 = pi_avg[x, a] / (1-pi_avg[x,a])\n",
    "                    ratio_ratio = ratio1/ratio2\n",
    "                    if ratio_ratio > worst_msm:\n",
    "                        worst_msm = ratio_ratio\n",
    "        print(worst_msm)\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        config = {'Gamma': worst_nky, 'lr':5 * 1e-2, 'epoch':200, 'nS': nStates, \n",
    "              'nA': nActions, 'bootstrap': True, 'n_bootstrap': 50}\n",
    "        lb_data = {'samps': nky_data, 'returns': returns}\n",
    "        eval_pol = (pi_e.T, pi_e.T)\n",
    "        test_lb = LB.loss_minimization(config=config, data=lb_data, \n",
    "                                           evaluation_policies=eval_pol, scope='toy_test')\n",
    "        test_lb_ours, test_lb_naive, _ = test_lb.run(use_tqdm=True)\n",
    "        print(test_lb_ours)\n",
    "        nky_results[i] = test_lb_ours.mean()\n",
    "\n",
    "        config = {'Gamma': worst_nky, 'lr':5 * 1e-2, 'epoch':200, 'nS': nStates, \n",
    "              'nA': nActions, 'bootstrap': False, 'n_bootstrap': 0}\n",
    "        lb_data = {'samps': nky_data, 'returns': returns}\n",
    "        eval_pol = (pi_e.T, pi_e.T)\n",
    "        test_lb = LB.loss_minimization(config=config, data=lb_data, \n",
    "                                           evaluation_policies=eval_pol, scope='toy_test')\n",
    "        test_lb_ours, test_lb_naive, _ = test_lb.run(use_tqdm=True)\n",
    "        print(\"nky no bootstrap\")\n",
    "        print(test_lb_ours)\n",
    "        #nky_results[i] = test_lb_ours.mean()\n",
    "\n",
    "        for j,P_bound in enumerate(Pbs):\n",
    "            fixed_u_v = confound_ope.fixed_u_gp_s_rect(V0, pi_e, 0.50, Phat, pihat, P_bound, worst_msm, mdp)\n",
    "            mb_results[i,j] = fixed_u_v @ mdp.x_dist\n",
    "            \n",
    "        true_gamma_mb_envs.append(mb_results)\n",
    "        true_gamma_nky_envs.append(nky_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_gamma_mb_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_gamma_nky_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force same $\\Gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mb_envs = []\n",
    "nky_envs = []\n",
    "\n",
    "gams = [1, 2, 4, 10]\n",
    "nGams = len(gams)\n",
    "\n",
    "Pbs = [10, 100000]\n",
    "#Pbs = [100000]\n",
    "nPbs = len(Pbs)\n",
    "\n",
    "hadds = [0]\n",
    "\n",
    "for mdp , pi_b, pi_e, base_horizon, gamma, nStates, nActions, term in envs:\n",
    "    \n",
    "    for h in hadds:\n",
    "        horizon = base_horizon + h\n",
    "\n",
    "        print(\"running env...\")\n",
    "        dataset = confound_mdp.collect_sample(200000, mdp, pi_b, horizon)\n",
    "        data = dataset.reshape((dataset.shape[0]*dataset.shape[1],5))\n",
    "        Phat = confound_ope.estimate_P(dataset, mdp)\n",
    "        pihat = confound_ope.estimate_pi(dataset, mdp)\n",
    "        for a in range(nActions):\n",
    "            for s in range(nStates):\n",
    "                if Phat[a,s].sum() == 0:\n",
    "                    Phat[a,s,term] = 1\n",
    "                if pihat[s].sum() == 0:\n",
    "                    pihat[s,:] = 1/nActions\n",
    "        pi_avg = pi_b[0] * u_dist[0] + pi_b[1] * u_dist[1]\n",
    "\n",
    "        print(\"value of pi_e with no confounding\")\n",
    "        Q0 = np.zeros((nStates, nActions))\n",
    "        nom_q = Q0.copy()\n",
    "        for t in range(horizon):\n",
    "            nom_q = confound_ope.fitted_q_update(nom_q, pi_e, dataset, mdp)\n",
    "        print(mdp.get_value(nom_q,pi_e)[1])\n",
    "\n",
    "        nky_data = transform_dataset(dataset)\n",
    "        returns = confound_mdp.calc_returns(dataset, gamma, horizon)\n",
    "       \n",
    "        nky_results = np.zeros((nGams))            \n",
    "        mb_results = np.zeros((nGams, nPbs))\n",
    "        \n",
    "        Q0 = np.zeros((nStates, nActions))\n",
    "        q_reparam_samp = Q0.copy()\n",
    "        for t in range(horizon-1):\n",
    "            q_reparam_samp = confound_ope.fitted_q_update(q_reparam_samp, pi_e, dataset, mdp)\n",
    "        V0 = mdp.get_value(q_reparam_samp,pi_e)[0]\n",
    "\n",
    "        for i,gam in enumerate(gams):\n",
    "            config = {'Gamma': gam, 'lr':5 * 1e-2, 'epoch':200, 'nS': nStates, \n",
    "                  'nA': nActions, 'bootstrap': True, 'n_bootstrap': 50}\n",
    "            lb_data = {'samps': nky_data, 'returns': returns}\n",
    "            eval_pol = (pi_e.T, pi_e.T)\n",
    "            test_lb = LB.loss_minimization(config=config, data=lb_data, \n",
    "                                               evaluation_policies=eval_pol, scope='toy_test')\n",
    "            test_lb_ours, test_lb_naive, _ = test_lb.run(use_tqdm=True)\n",
    "            print(test_lb_ours)\n",
    "            nky_results[i] = test_lb_ours.mean()\n",
    "            \n",
    "            config = {'Gamma': gam, 'lr':5 * 1e-2, 'epoch':200, 'nS': nStates, \n",
    "                  'nA': nActions, 'bootstrap': False, 'n_bootstrap': 0}\n",
    "            lb_data = {'samps': nky_data, 'returns': returns}\n",
    "            eval_pol = (pi_e.T, pi_e.T)\n",
    "            test_lb = LB.loss_minimization(config=config, data=lb_data, \n",
    "                                               evaluation_policies=eval_pol, scope='toy_test')\n",
    "            test_lb_ours, test_lb_naive, _ = test_lb.run(use_tqdm=True)\n",
    "            print(\"nky no bootstrap\")\n",
    "            print(test_lb_ours)\n",
    "            #nky_results[i] = test_lb_ours.mean()\n",
    "            \n",
    "            for j,P_bound in enumerate(Pbs):\n",
    "                fixed_u_v = confound_ope.fixed_u_gp_s_rect(V0, pi_e, 0.50, Phat, pihat, P_bound, gam, mdp)\n",
    "                mb_results[i,j] = fixed_u_v @ mdp.x_dist\n",
    "            \n",
    "        mb_envs.append(mb_results)\n",
    "        nky_envs.append(nky_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nky_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump( mb_envs, open( \"mb_namkoong_compare_experiments.p\", \"wb\" ) )\n",
    "# pickle.dump( nky_envs, open( \"nky_namkoong_compare_experiments.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
